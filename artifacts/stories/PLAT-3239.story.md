# Fix actions.api Logging / Parsing for ELK

## Description

ELK logs for `actions.api` don't parse correctly - users see raw JSON dumps that aren't separated by entry, tagged with `_grokparsefailure`. This significantly reduces ELK's effectiveness for debugging one of our most critical components. The root cause is that console output consumed via AWS FireLens assumes standard HTTP logging format, but ad hoc log entries don't follow this format and aren't constrained to single lines.

**Scope Expansion:** This story also establishes the infrastructure pattern for direct Elasticsearch logging that can be adopted by other applications (actions.manager, legacy apps), eventually replacing Filebeat-based log shipping. This includes:
- Environment-specific logging configuration (local dev vs ECS deployments)
- Local ELK stack setup for development
- GitHub Actions configuration for deployment environments

---

## Metadata

| Field | Value |
|-------|-------|
| **Story ID** | PLAT-3239 |
| **JIRA Issue** | [PLAT-3239](https://resolvesys.atlassian.net/browse/PLAT-3239) |
| **Status** | In Progress |
| **Priority** | P2_High |
| **Story Points** | 16 |
| **Sprint** | Ready To Groom |
| **Created** | 2025-12-05 |
| **Owner** | Dev Team |

---

## Status

**In Progress** - Core implementation complete on `feature/PLAT-3239-elk-logging` branch (11 commits). Performance benchmarking complete with drop monitoring capability. Pending: staging deployment and E2E validation.

---

## User Story

**As a** developer or operations engineer,
**I want** actions.api logs to be properly parsed and indexed in ELK,
**So that** I can effectively search, filter, and analyze both HTTP response logs and ad hoc application logs without encountering grok parse failures.

---

## Acceptance Criteria

1. **Given** the actions.api is running in ECS, **When** standard HTTP requests are logged, **Then** they continue to appear in `ecs-logs` index via FireLens with proper field parsing (no regression)

2. **Given** the actions.api logs ad hoc entries (via `Log.Information`, `Log.Warning`, etc.), **When** these entries are shipped to ELK, **Then** they appear in the `application-logs` index as properly parsed individual entries

3. **Given** ad hoc log entries are generated, **When** they are processed by Serilog, **Then** they are NOT written to console (avoiding `_grokparsefailure` in `ecs-logs`)

4. **Given** a multi-line log entry (e.g., exception with stack trace), **When** it is shipped to ELK, **Then** it appears as a single cohesive log entry, not split across multiple entries

5. **Given** the Elasticsearch sink is configured, **When** the actions.api starts, **Then** it connects to Elasticsearch and begins shipping logs without errors

6. **Given** Elasticsearch is temporarily unavailable, **When** the actions.api attempts to ship logs, **Then** it buffers logs locally and retries without crashing or losing log data

7. **Given** logs appear in `application-logs` index, **When** compared to existing entries from actions.manager and legacy apps, **Then** the field structure is consistent/compatible for unified querying

---

## Assumptions

- Production Elasticsearch cluster version is 7.17.15 (confirmed via Kibana Stack Management)
- Local ELK stack exists at `D:\dev\resolve.dev.resources\Docker Containers\elastic-stack` (needs version update from 8.7.1 → 7.17.15)
- Elasticsearch endpoint is accessible from ECS containers (network/security group configuration exists)
- ELK index lifecycle management (ILM) policies are already in place or can be configured
- Existing FireLens/console logging for HTTP responses remains unchanged in ECS (HTTP logs → `ecs-logs`)
- Ad hoc logs will go to `application-logs` index (same as actions.manager, legacy apps)
- Configuration follows established token replacement pattern (like `apiClient.json`, `rabbitmq.json`)
- GitHub Actions already have environment-specific variable injection pattern to copy
- This pattern may be adopted by actions.manager and other apps in the future (retiring Filebeat)

### Environment-Specific Behavior

| Environment | Console Output | ES Sink Target | Notes |
|-------------|----------------|----------------|-------|
| **Local Dev** | HTTP only (filtered) | Local ELK (localhost:9200) | Orca shows console, direct ES sink |
| **Dev/Staging/Prod (ECS)** | HTTP only (filtered) | Production ELK | FireLens for HTTP, direct ES sink |

**Key Insight:** Behavior is IDENTICAL across all environments - only the ES URL differs. This ensures local development accurately mirrors production behavior.

---

## Dependencies

- `Serilog.Sinks.Elasticsearch` NuGet package (v9.x or v10.x - supports ES 7.x)
- `Serilog.Sinks.Async` NuGet package (for non-blocking logging)
- `Serilog.Expressions` NuGet package (for sink filtering)
- Elasticsearch 7.17.15 cluster (confirmed)
- Network connectivity from ECS to Elasticsearch endpoint
- No blocking dependencies on other stories
- **Future:** When cluster upgrades to ES 8.x, migrate to `Elastic.Serilog.Sinks`

---

## Definition of Done

- [ ] All acceptance criteria met and validated
- [ ] All tasks completed
- [ ] Code implemented following PRISM principles
- [ ] Unit tests written and passing
- [ ] Integration tested in dev/staging ECS environment
- [ ] ELK queries verified: ad hoc logs searchable without grok failures
- [ ] Code reviewed and approved
- [ ] No regression in existing HTTP response logging
- [ ] Documentation updated (if configuration changes required)

---

## Tasks / Subtasks

### Phase 1: Core Implementation (actions.api)

#### Task 1: Add NuGet Packages (AC: 5)
- [x] Add `Serilog.Sinks.Elasticsearch` package (v9.x or v10.x) to `actions.api.csproj` *(pivoted to Serilog.Sinks.Http)*
- [x] Add `Serilog.Sinks.Async` package (for non-blocking logging)
- [x] Add `Serilog.Expressions` package (for sink filtering)
- [x] Verify package version compatibility with .NET version and existing Serilog packages
- [x] Build solution to confirm no package conflicts

#### Task 2: Create Logging Configuration File (AC: 5)
- [ ] Create `Configurations/logging.json` following established pattern (like `apiClient.json`):
  ```json
  {
    "logging": {
      "elasticsearchUrl": "%LOGGING_ELASTICSEARCH_URL%",
      "indexFormat": "application-logs-{0:yyyy.MM.dd}"
    }
  }
  ```
- [ ] Add `Configuration.Logging.cs` to load and bind configuration
- [ ] Support Orca service discovery as primary (if applicable), fallback to config file
- [ ] **Unit Test:** Verify configuration loads correctly

#### Task 3: Configure Console Sink - HTTP Logs Only (AC: 1, 3)
- [ ] Configure Console sink to output HTTP request logs ONLY (all environments)
  - Filter: `SourceContext = 'Serilog.AspNetCore.RequestLoggingMiddleware'`
- [ ] Verify ad hoc logs do NOT go to console
- [ ] Update `Configuration.Logger.cs` to apply filtering
- [ ] **Note:** Behavior is identical across all environments (local/ECS)
- [ ] **Unit Test:** Verify console receives only HTTP request logs

#### Task 4: Configure Elasticsearch Sink with Async Wrapper (AC: 2, 4, 6)
- [x] Wrap BOTH sinks (Console + ES) with `Serilog.Sinks.Async` for non-blocking logging
- [x] Configure async buffer limits to prevent memory pressure:
  - `bufferSize`: 1000 (reduced from 10K default)
  - `blockWhenFull`: false (drop logs rather than block request threads)
- [x] Configure destructuring limits to prevent large object serialization:
  - `maximumDestructuringDepth`: 4
  - `maximumStringLength`: 1024
  - `maximumCollectionCount`: 10
- [x] Add Elasticsearch sink to Serilog pipeline (enabled in ALL environments) *(via HTTP sink with bulk API)*
- [x] Target index: `application-logs-{yyyy.MM.dd}` (matches existing infrastructure)
- [x] Filter to EXCLUDE HTTP request logs (avoid duplication with console/FireLens)
- [x] ES URL loaded from `logging.json` (localhost for local, production URL for ECS)
- [x] Configure resilient connection:
  - Durable file buffer: `./logs/elasticsearch-buffer`
  - Buffer size limit: 5MB
  - Batch posting: 50 events / 5 seconds
- [ ] **Unit Test:** Verify ES sink receives ad hoc logs only

#### Task 5: Match Existing application-logs Index Format (AC: 7)
- [ ] Query `application-logs` index in Kibana for recent actions.manager entries
- [ ] Document actions.manager field structure (timestamp, level, message, exception, etc.)
- [ ] Compare Serilog.Sinks.Elasticsearch output format with actions.manager format
- [ ] Configure JSON formatter to match existing format
- [ ] Ensure field names align (e.g., `@timestamp` vs `timestamp`)
- [ ] **Integration Test:** New logs query alongside existing logs correctly

#### Task 6: Performance Benchmarking
- [x] Create benchmark test harness (can be simple console app or integration test)
- [x] **Baseline measurement (sync):** Temporarily disable async wrapper
  - Measure request latency percentiles (p50, p95, p99) under load
  - Measure logging throughput (logs/second)
- [x] **Async measurement:** Enable async wrapper
  - Same metrics as baseline
  - Document improvement in request latency
- [x] **Large exception test:**
  - Generate exceptions with 100KB+ serialized size (deep stack traces, large Data dict)
  - Verify destructuring limits truncate appropriately
  - Monitor memory usage during sustained exception logging
  - ✓ PASS: 1,687 exceptions/sec, memory bounded <50MB
- [x] **Buffer overflow test:**
  - Simulate slow/unavailable ES (e.g., pause container)
  - Verify `blockWhenFull: false` drops logs without blocking
  - Verify no memory growth beyond buffer limit
  - ✓ PASS: 391,876 logs/sec (non-blocking), memory bounded <20MB
- [x] **Document results:** Benchmark suite generates JSON reports with metrics + markdown reports
- [x] **Acceptance:** Async shows measurable latency improvement, no memory issues
  - ✓ Throughput: 157K-617K logs/sec depending on sink type
  - ✓ Drop monitoring: `IAsyncLogEventSinkMonitor` tracks dropped events
  - ✓ Memory bounded during all stress tests

---

### Phase 2: Local Development Environment

#### Task 7: Update Local ELK Stack Version
- [ ] Location: `D:\dev\resolve.dev.resources\Docker Containers\elastic-stack`
- [ ] Update `sample.env.txt`: Change `STACK_VERSION` from 8.7.1 to 7.17.15
- [ ] Test docker-compose up with new version
- [ ] Verify Elasticsearch starts and is accessible at localhost:9200
- [ ] Verify Kibana starts and is accessible at localhost:5601
- [ ] Document any breaking changes or migration steps
- [ ] **Note:** Filebeat NOT used for actions.api (direct ES sink instead)
- [ ] **Verification:** Local ELK matches production version

#### Task 8: Create Local Development Logging Config
- [ ] Create `logging.Development.json` with localhost ES URL:
  ```json
  {
    "logging": {
      "elasticsearchUrl": "http://localhost:9200"
    }
  }
  ```
- [ ] Verify actions.api uses ES sink locally (same as ECS environments)
- [ ] Verify console shows HTTP request logs only (viewable in Orca)
- [ ] Verify ad hoc logs appear in local Kibana `application-logs-*` index
- [ ] Document local development setup
- [ ] **Verification:** Local dev logging mirrors production behavior

---

### Phase 3: Deployment Configuration

#### Task 9: Add GitHub Variables/Secrets
- [ ] Add to GitHub repository variables:
  - `LOGGING_ELASTICSEARCH_URL_DEV`
  - `LOGGING_ELASTICSEARCH_URL_STAGING`
  - `LOGGING_ELASTICSEARCH_URL_PROD`
- [ ] Add to GitHub repository secrets (if credentials needed):
  - `LOGGING_ELASTICSEARCH_USERNAME_*` (if applicable)
  - `LOGGING_ELASTICSEARCH_PASSWORD_*` (if applicable)
- [ ] Document variable names and expected values
- [ ] **⚠️ DEVOPS:** May require admin access to GitHub repository settings

#### Task 10: Update GitHub Actions Workflows
- [ ] Update `dev.yml` workflow:
  - Add environment variable injection for logging config
  - Add PowerShell token replacement for `logging.json`
- [ ] Update `staging.yml` workflow (same pattern)
- [ ] Update `production.yml` workflow (same pattern)
- [ ] Follow existing pattern from `apiClient.json` / `rabbitmq.json` replacement
- [ ] **⚠️ DEVOPS:** Requires PR to actions.api repository workflows

#### Task 11: Update ECS Task Definition (If Required)
- [ ] Determine if ES URL should come from environment variable or Secrets Manager
- [ ] Update ECS task definition with logging environment variables
- [ ] Verify network connectivity from ECS to Elasticsearch
- [ ] **⚠️ DEVOPS:** May require Terraform/CloudFormation changes

---

### Phase 4: Integration Testing & Validation

#### Task 12: Local Development Testing
- [ ] Start local ELK stack via docker-compose
- [ ] Run actions.api locally (via Orca or direct)
- [ ] Generate HTTP requests → verify logs appear in console (HTTP only)
- [ ] Generate ad hoc logs → verify logs appear in local Kibana (NOT console)
- [ ] Verify `application-logs-*` index populated in local Kibana
- [ ] Verify console shows ONLY HTTP request logs (no ad hoc)
- [ ] **Verification:** Local dev workflow complete, behavior matches ECS

#### Task 13: ECS Environment Testing (AC: 1, 2, 3, 4)
- [ ] Deploy to dev ECS environment
- [ ] Generate HTTP requests → verify logs in `ecs-logs` via FireLens
- [ ] Generate ad hoc logs → verify logs in `application-logs` via ES sink
- [ ] Verify NO ad hoc logs in `ecs-logs` (no `_grokparsefailure`)
- [ ] Verify NO HTTP logs duplicated in `application-logs`
- [ ] Test multi-line exception logging
- [ ] **Integration Test:** Both pipelines working in isolation

#### Task 14: Production Validation
- [ ] Deploy to staging environment
- [ ] Verify logs appear in production ELK `application-logs` index
- [ ] Compare format with existing actions.manager logs
- [ ] Monitor for any performance impact
- [ ] **Sign-off:** Ready for production deployment

---

### Phase 5: Orca ELK Stack Orchestration

**Goal:** Automatically start the full local ELK stack (Elasticsearch, Kibana, Logstash, Filebeat) when running Orca locally, with TLS and authentication enabled to match production.

**Architecture:** Init container pattern with volume caching - certificates generated on first run (~60s), cached in Docker volume for instant subsequent startups.

**Environment Scope:** Local development only. Cloud dev environments use production ELK stack.

```csharp
// ELK stack only runs in local development (not CI/cloud)
if (builder.Environment.IsDevelopment()
    && string.IsNullOrEmpty(Environment.GetEnvironmentVariable("GITHUB_ACTIONS"))
    && string.IsNullOrEmpty(Environment.GetEnvironmentVariable("CI")))
{
    // Add ELK containers here...
}
```

| Environment | ELK Source | Notes |
|-------------|------------|-------|
| **Local dev** | Orca-managed containers | ES, Kibana, Logstash, Filebeat via Aspire |
| **Cloud dev** | Production ELK | Logs ship to shared ELK cluster |
| **CI/GitHub Actions** | None | No ELK needed for automated tests |

#### Task 15: Add ELK Setup/Init Containers
- [x] Wrap all ELK containers in local-dev-only conditional (same pattern as express-web-api/client)
- [x] Add setup container to `Orca.AppHost\Program.cs` that generates TLS certificates:
  ```csharp
  // ELK Stack Parameters
  var elasticPassword = builder.AddParameter("elastic-password", "R3solv3!", secret: true);
  var kibanaPassword = builder.AddParameter("kibana-password", "R3solv3!", secret: true);
  var kibanaEncryptionKey = builder.AddParameter("kibana-encryption-key", "a".PadRight(32, 'a'), secret: true);

  // Setup container - generates certs on first run, skips if already exist
  var elkSetup = builder.AddContainer("setup", "docker.elastic.co/elasticsearch/elasticsearch:7.17.15")
      .WithContainerName("elk-setup")
      .WithVolume("elastic-certs", "/usr/share/elasticsearch/config/certs")
      .WithEnvironment("ELASTIC_PASSWORD", elasticPassword)
      .WithEnvironment("KIBANA_PASSWORD", kibanaPassword)
      .WithBindMount(elkSetupScriptPath, "/setup.sh", isReadOnly: true)
      .WithEntrypoint("/bin/bash")
      .WithArgs("/setup.sh")
      .WithParentRelationship(elasticsearch)
      .WithLifetime(ContainerLifetime.Session);  // Runs once per Orca session
  ```
- [x] Add post-setup container that sets `kibana_system` password after ES is ready:
  ```csharp
  // Post-setup container: waits for ES to be ready and sets kibana_system password
  var elkPostSetup = builder.AddContainer("post-setup", "docker.elastic.co/elasticsearch/elasticsearch:7.17.15")
      .WithContainerName("elk-post-setup")
      .WithVolume("elastic-certs", "/usr/share/elasticsearch/config/certs")
      .WithBindMount(elkPostSetupPath, "/post-setup.sh", isReadOnly: true)
      .WithEnvironment("ELASTIC_PASSWORD", elasticPassword)
      .WithEnvironment("KIBANA_PASSWORD", kibanaPassword)
      .WithEntrypoint("/bin/bash")
      .WithArgs("/post-setup.sh")
      .WithParentRelationship(elasticsearch)
      .WaitFor(elasticsearch)
      .WithLifetime(ContainerLifetime.Session);
  ```
- [x] Implement cert generation script (CA + node certs for elasticsearch, kibana)
- [x] Implement post-setup script for kibana_system password
- [x] Test first-run cert generation (~60s)
- [x] Test subsequent runs use cached certs (instant)

#### Task 16: Add Elasticsearch Container with TLS
- [x] Add Elasticsearch 7.17.15 container with full security enabled:
  ```csharp
  var elasticsearch = builder.AddContainer("elasticsearch", "docker.elastic.co/elasticsearch/elasticsearch:7.17.15")
      .WithContainerName("elk-elasticsearch")
      .WithVolume("elastic-certs", "/usr/share/elasticsearch/config/certs")
      .WithVolume("esdata01", "/usr/share/elasticsearch/data")
      .WithEnvironment("node.name", "elasticsearch")
      .WithEnvironment("cluster.name", "docker-cluster")
      .WithEnvironment("discovery.type", "single-node")
      .WithEnvironment("ELASTIC_PASSWORD", elasticPassword)
      .WithEnvironment("bootstrap.memory_lock", "true")
      .WithEnvironment("xpack.security.enabled", "true")
      .WithEnvironment("xpack.security.http.ssl.enabled", "true")
      .WithEnvironment("xpack.security.http.ssl.key", "certs/elasticsearch/elasticsearch.key")
      .WithEnvironment("xpack.security.http.ssl.certificate", "certs/elasticsearch/elasticsearch.crt")
      .WithEnvironment("xpack.security.http.ssl.certificate_authorities", "certs/ca/ca.crt")
      .WithEnvironment("xpack.security.transport.ssl.enabled", "true")
      .WithEnvironment("xpack.security.transport.ssl.key", "certs/elasticsearch/elasticsearch.key")
      .WithEnvironment("xpack.security.transport.ssl.certificate", "certs/elasticsearch/elasticsearch.crt")
      .WithEnvironment("xpack.security.transport.ssl.certificate_authorities", "certs/ca/ca.crt")
      .WithEnvironment("xpack.security.transport.ssl.verification_mode", "certificate")
      .WithEnvironment("xpack.license.self_generated.type", "basic")
      .WithEnvironment("ES_JAVA_OPTS", "-Xms1g -Xmx1g")
      .WithEndpoint(port: 9200, targetPort: 9200, isProxied: false, name: "https")
      .WithLifetime(ContainerLifetime.Persistent);

  // Wire up elasticsearch to wait for setup completion
  elasticsearch.WaitForCompletion(elkSetup);
  ```
- [x] Add health check: `curl -s --cacert certs/ca/ca.crt https://localhost:9200`
- [x] Test ES starts with TLS enabled
- [x] Verify authentication required: `curl https://localhost:9200` returns 401

#### Task 17: Add Kibana Container with TLS
- [x] Add Kibana 7.17.15 container connecting to ES via HTTPS:
  ```csharp
  var kibana = builder.AddContainer("kibana", "docker.elastic.co/kibana/kibana:7.17.15")
      .WithContainerName("elk-kibana")
      .WithVolume("elastic-certs", "/usr/share/kibana/config/certs")
      .WithVolume("kibanadata", "/usr/share/kibana/data")
      .WithEnvironment("SERVERNAME", "kibana")
      .WithEnvironment("ELASTICSEARCH_HOSTS", "https://elk-elasticsearch:9200")
      .WithEnvironment("ELASTICSEARCH_USERNAME", "kibana_system")
      .WithEnvironment("ELASTICSEARCH_PASSWORD", kibanaPassword)
      .WithEnvironment("ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES", "config/certs/ca/ca.crt")
      .WithEnvironment("XPACK_SECURITY_ENCRYPTIONKEY", kibanaEncryptionKey)
      .WithEnvironment("XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY", kibanaEncryptionKey)
      .WithEnvironment("XPACK_REPORTING_ENCRYPTIONKEY", kibanaEncryptionKey)
      .WithHttpEndpoint(port: 5601, targetPort: 5601, name: "http")
      .WithParentRelationship(elasticsearch)
      .WaitFor(elasticsearch)
      .WaitForCompletion(elkPostSetup)
      .WithLifetime(ContainerLifetime.Persistent);
  ```
- [x] Test Kibana UI accessible at `http://localhost:5601`
- [x] Verify Kibana connects to ES with kibana_system user
- [x] Test login with elastic/R3solv3! credentials

#### Task 18: Add Logstash Container
- [x] Add Logstash 7.17.15 container:
  ```csharp
  var logstash = builder.AddContainer("logstash", "docker.elastic.co/logstash/logstash:7.17.15")
      .WithContainerName("elk-logstash")
      .WithVolume("elastic-certs", "/usr/share/logstash/certs")
      .WithVolume("logstashdata01", "/usr/share/logstash/data")
      .WithBindMount(logstashConfPath, "/usr/share/logstash/pipeline/logstash.conf", isReadOnly: true)
      .WithEnvironment("xpack.monitoring.enabled", "false")
      .WithEnvironment("ELASTIC_USER", "elastic")
      .WithEnvironment("ELASTIC_PASSWORD", elasticPassword)
      .WithEnvironment("ELASTIC_HOSTS", "https://elk-elasticsearch:9200")
      .WithParentRelationship(elasticsearch)
      .WaitFor(elasticsearch)
      .WaitFor(kibana)
      .WithLifetime(ContainerLifetime.Persistent);
  ```
- [x] Copy `logstash.conf` from resolve.dev.resources to Orca.AppHost/elk/
- [x] Verify Logstash connects to ES

#### Task 19: Add Filebeat Container
- [x] Add Filebeat 7.17.15 container:
  ```csharp
  var filebeat = builder.AddContainer("filebeat", "docker.elastic.co/beats/filebeat:7.17.15")
      .WithContainerName("elk-filebeat")
      .WithVolume("elastic-certs", "/usr/share/filebeat/certs")
      .WithVolume("filebeatdata01", "/usr/share/filebeat/data")
      .WithBindMount(filebeatYmlPath, "/usr/share/filebeat/filebeat.yml", isReadOnly: true)
      .WithBindMount(devRoot, "/usr/share/filebeat/ingest_root_dev")
      .WithEnvironment("ELASTIC_USER", "elastic")
      .WithEnvironment("ELASTIC_PASSWORD", elasticPassword)
      .WithEnvironment("ELASTIC_HOSTS", "https://elk-elasticsearch:9200")
      .WithEnvironment("KIBANA_HOSTS", "http://elk-kibana:5601")
      .WithArgs("--strict.perms=false")
      .WithParentRelationship(elasticsearch)
      .WaitFor(elasticsearch)
      .WithLifetime(ContainerLifetime.Persistent);
  ```
- [x] Copy `filebeat.yml` from resolve.dev.resources to Orca.AppHost/elk/
- [x] Configure filebeat to monitor `actions.manager/**/Logs/`
- [x] Verify Filebeat ships logs to ES

#### Task 20: Documentation and Integration Testing
- [x] Update Orca README with ELK stack information:
  - First-run behavior (cert generation ~60s)
  - Credentials (elastic/R3solv3!)
  - URLs (ES: https://localhost:9200, Kibana: http://localhost:5601)
- [x] Test full workflow:
  - Fresh start (no volumes) → certs generated → all services start
  - Subsequent start → instant startup (cached certs)
  - actions.api logs appear in Kibana `application-logs-*` index
  - actions.manager logs appear via Filebeat
- [x] Verify all containers appear in Aspire dashboard (grouped under elasticsearch parent)
- [x] **Note:** `resolve.dev.resources\Docker Containers\elastic-stack` remains as reference/sample

---

## Dev Notes

### Current Architecture (from codebase exploration)

**Project Location:** `D:\dev\actions.api\src`

**Current Serilog Configuration:**
- **Main Config:** `Configurations/logger.json`
- **Dev Override:** `appsettings.development.json` (Console sink)
- **Docker Override:** `appsettings.docker.json` (Seq sink)
- **Setup:** `Configurations/Configuration.Logger.cs`

**Current Sinks:**
| Sink | Environment | Min Level |
|------|-------------|-----------|
| Console | Development | All |
| File | Production | Warning+ |
| Seq | Docker | All |
| Hangfire | All | All |

**Current Enrichers:**
- FromLogContext, WithMachineName, WithProcessId, WithThreadId
- WithHangfireContext, WithExceptionDetails
- HTTP Request enrichment (IP, Host, Protocol, Scheme, User)

### Root Cause Analysis

**Problem:** AWS FireLens consumes console output assuming standard HTTP logging format defined in:
`https://github.com/resolve-io/deployment-environments/blob/a8efee49f14ad3ccbe5b5a04ab0cdb06ea0af22d/roles/ansible-role-elk-server/templates/http-input-ecs.conf.j2#L18`

**Why Ad Hoc Logs Fail:**
1. Ad hoc logs don't follow the HTTP response format
2. Ad hoc logs may span multiple lines (especially exceptions)
3. Grok pattern expects specific format, fails on anything else
4. Failed entries tagged with `_grokparsefailure`, raw JSON dumped to `message` field

### Solution: Environment-Aware Dual-Sink Architecture

**Approach:** Use Serilog sink filtering with environment-specific behavior:

#### ECS Environments (Dev/Staging/Production)
```
┌─────────────────────────────────────────────────────────────┐
│                    Serilog Pipeline                         │
│  (Log.Information, Log.Warning, HTTP Request Logging)       │
└─────────────────────────┬───────────────────────────────────┘
                          │
                    ┌─────┴─────┐
                    │  Filters  │
                    └─────┬─────┘
                          │
          ┌───────────────┴───────────────┐
          │                               │
          ▼                               ▼
┌─────────────────────┐       ┌─────────────────────────┐
│   Console Sink      │       │   Elasticsearch Sink    │
│                     │       │                         │
│ Filter: ONLY        │       │ Filter: EXCLUDE         │
│ HTTP Request logs   │       │ HTTP Request logs       │
└──────────┬──────────┘       └────────────┬────────────┘
           │                               │
           ▼                               ▼
      AWS FireLens                  Production ELK
           │                      (elastic.resolve.io)
           ▼                               │
       ecs-logs                            ▼
        index                      application-logs
                                        index
```

#### Local Development Environment
```
┌─────────────────────────────────────────────────────────────┐
│                    Serilog Pipeline                         │
│  (Log.Information, Log.Warning, HTTP Request Logging)       │
└─────────────────────────┬───────────────────────────────────┘
                          │
                    ┌─────┴─────┐
                    │  Filters  │
                    └─────┬─────┘
                          │
          ┌───────────────┴───────────────┐
          │                               │
          ▼                               ▼
┌─────────────────────┐       ┌─────────────────────────┐
│   Console Sink      │       │   Elasticsearch Sink    │
│                     │       │                         │
│ Filter: ONLY        │       │ Filter: EXCLUDE         │
│ HTTP Request logs   │       │ HTTP Request logs       │
└──────────┬──────────┘       └────────────┬────────────┘
           │                               │
           ▼                               ▼
      Orca Console                   Local ELK Stack
      (dev visibility)              (localhost:9200)
                                           │
                                           ▼
                                   application-logs
                                        index
```

**Key: Behavior is IDENTICAL to ECS environments - only the ES URL differs.**

**Benefits:**
1. HTTP request logs continue working via existing FireLens pipeline (no regression)
2. Ad hoc logs bypass FireLens/grok entirely - no parse failures
3. Multi-line entries (exceptions) handled correctly as single documents
4. Durable buffering - logs buffered to disk if ES temporarily unavailable
5. Consistent with `application-logs` index format (actions.manager, legacy apps)
6. **Identical behavior** across local dev and ECS - only ES URL differs
7. Local ELK stack mirrors production for testing and debugging
8. **Future-proof:** Pattern can be adopted by other apps, eventually retiring Filebeat

### Package Choice: Serilog.Sinks.Elasticsearch

**Cluster Version:** Elasticsearch 7.17.15 (confirmed via Kibana Stack Management)

**Why Serilog.Sinks.Elasticsearch:**
- ✅ Supports Elasticsearch 7.x (our cluster version)
- ✅ Stable, production-proven package
- ✅ Rich configuration options for index templates, buffering, retry
- ✅ Works with existing Serilog JSON configuration pattern
- ⚠️ Package is archived (no new features) but existing versions work fine

**Why NOT Elastic.Serilog.Sinks (for now):**
- ❌ Requires Elasticsearch 8.15.0+ (we have 7.17.15)
- ❌ Would not connect to our cluster

**Future Migration Path:**
When the Elasticsearch cluster is upgraded to 8.x, consider migrating to `Elastic.Serilog.Sinks`:
- Officially supported by Elastic
- Uses ECS logging specification (native format)
- Uses data streams and ILM by default (modern best practices)

### Configuration Example

Complete dual-sink configuration with async wrappers and memory protection:

```json
{
  "Serilog": {
    "Using": [
      "Serilog.Sinks.Async",
      "Serilog.Sinks.Elasticsearch",
      "Serilog.Expressions"
    ],
    "Enrich": [
      "FromLogContext",
      "WithMachineName",
      "WithProcessId",
      "WithThreadId"
    ],
    "Destructure": [
      { "Name": "ToMaximumDepth", "Args": { "maximumDestructuringDepth": 4 } },
      { "Name": "ToMaximumStringLength", "Args": { "maximumStringLength": 1024 } },
      { "Name": "ToMaximumCollectionCount", "Args": { "maximumCollectionCount": 10 } }
    ],
    "WriteTo": [
      {
        "Name": "Async",
        "Args": {
          "bufferSize": 1000,
          "blockWhenFull": false,
          "configure": [
            {
              "Name": "Logger",
              "Args": {
                "configureLogger": {
                  "Filter": [
                    {
                      "Name": "ByIncludingOnly",
                      "Args": {
                        "expression": "SourceContext = 'Serilog.AspNetCore.RequestLoggingMiddleware'"
                      }
                    }
                  ],
                  "WriteTo": [{ "Name": "Console" }]
                }
              }
            }
          ]
        }
      },
      {
        "Name": "Async",
        "Args": {
          "bufferSize": 1000,
          "blockWhenFull": false,
          "configure": [
            {
              "Name": "Logger",
              "Args": {
                "configureLogger": {
                  "Filter": [
                    {
                      "Name": "ByExcluding",
                      "Args": {
                        "expression": "SourceContext = 'Serilog.AspNetCore.RequestLoggingMiddleware'"
                      }
                    }
                  ],
                  "WriteTo": [
                    {
                      "Name": "Elasticsearch",
                      "Args": {
                        "nodeUris": "%LOGGING_ELASTICSEARCH_URL%",
                        "indexFormat": "application-logs-{0:yyyy.MM.dd}",
                        "autoRegisterTemplate": true,
                        "autoRegisterTemplateVersion": "ESv7",
                        "numberOfShards": 2,
                        "numberOfReplicas": 1,
                        "batchPostingLimit": 50,
                        "period": "00:00:05",
                        "bufferBaseFilename": "./logs/elasticsearch-buffer",
                        "bufferFileSizeLimitBytes": 5242880
                      }
                    }
                  ]
                }
              }
            }
          ]
        }
      }
    ]
  }
}
```

**Key Performance Settings:**
| Setting | Value | Purpose |
|---------|-------|---------|
| `bufferSize` | 1000 | Limits in-memory event queue (vs 10K default) |
| `blockWhenFull` | false | Drop logs rather than block request threads |
| `maximumDestructuringDepth` | 4 | Prevents deep object graph serialization |
| `maximumStringLength` | 1024 | Truncates large string properties |
| `maximumCollectionCount` | 10 | Limits array/collection serialization |

### Non-Blocking Mode Behavior

The async sink's `blockWhenFull` parameter controls behavior when the internal buffer reaches capacity:

| Mode | `blockWhenFull` | Behavior | Use Case |
|------|-----------------|----------|----------|
| **Non-blocking** | `false` | Drops new log events when buffer is full | API servers where latency matters more than log completeness |
| **Blocking** | `true` | Blocks caller thread until buffer has space | Batch jobs where log completeness matters more than throughput |

**Non-blocking mode trade-offs:**
- **Pro:** Request threads never block on logging - application latency unaffected by slow sink
- **Pro:** Memory bounded - buffer size limits memory growth during log bursts
- **Con:** Log events may be silently dropped during high load or slow sink
- **Mitigation:** Use `IAsyncLogEventSinkMonitor` to track dropped events (see benchmark suite)

**Buffer flushing:** Events in a partially-filled buffer are flushed on:
1. The HTTP sink's `period` timer (default 5 seconds)
2. When `Logger.Dispose()` is called (application shutdown)

### Destructuring Limits and JSON Safety

Serilog's destructuring limits protect against unbounded serialization of complex objects:

```csharp
.Destructure.ToMaximumDepth(4)           // Stop at 4 levels deep
.Destructure.ToMaximumStringLength(1024) // Truncate strings > 1024 chars
.Destructure.ToMaximumCollectionCount(10) // Only serialize first 10 items
```

**How they work:**
- Objects deeper than `maximumDestructuringDepth` are rendered as type names (e.g., `"MyClass"`)
- Strings longer than `maximumStringLength` are truncated with `...` suffix
- Collections with more than `maximumCollectionCount` items show only the first N items

**JSON validity:** Output is always valid JSON. Truncated content uses Serilog's standard rendering which produces valid JSON strings. The limits prevent:
- Memory pressure from serializing large exception Data dictionaries
- Slow serialization of deeply nested objects
- Log entries consuming excessive storage

**Benchmark results:** With these limits, the benchmark suite processed 1,687 large exceptions/sec (each with 100KB+ Data dictionaries) while keeping memory bounded under 50MB.

### Index Naming

Using `application-logs` index to match existing infrastructure:
- **Format:** `application-logs-{yyyy.MM.dd}`
- **Example:** `application-logs-2025.12.05`

This places logs alongside existing entries from:
- actions.manager (via Filebeat)
- Legacy apps (via Filebeat + log4net)

### Local ELK Stack Details

**Location:** `D:\dev\resolve.dev.resources\Docker Containers\elastic-stack`

**Current State:**
- ES/Kibana version: 7.17.15 (already updated to match production)
- Services: Elasticsearch, Kibana, Filebeat, Logstash
- Filebeat monitors: `actions.manager/**/Logs/` (NOT used for actions.api)

**Two Options for Local ELK:**

| Option | Use Case | How to Start |
|--------|----------|--------------|
| **Orca-managed (Phase 5)** | Standard local dev | `dotnet run` in Orca.AppHost |
| **Standalone docker-compose** | Advanced scenarios, debugging | `docker-compose up` in elastic-stack folder |

**Note:** Both options target ES 7.17.15 to match production. The standalone docker-compose includes TLS/security enabled; Orca-managed version uses simplified security for easier local dev.

### Orca ELK Orchestration (Phase 5)

**Integration Pattern:** Uses .NET Aspire's `AddContainer()` API, same as Redis, RabbitMQ, and Keycloak.

**Key Aspire Concepts:**
- `WithLifetime(ContainerLifetime.Persistent)` - Container survives app restarts
- `WithVolume("name", "/path")` - Named volumes for data persistence
- `WaitFor(resource)` - Dependency ordering (Kibana waits for ES, actions.api waits for ES)
- `WithEndpoint()` / `WithHttpEndpoint()` - Port mappings

**Simplified Security for Local Dev:**
The Orca-managed ELK stack disables X-Pack security (`xpack.security.enabled=false`) for simpler local development. This differs from the standalone docker-compose which uses full TLS.

**Container Names:**
- `elasticsearch-dev` (port 9200)
- `kibana-dev` (port 5601)

**Volume Names:**
- `esdata-dev` - Elasticsearch data
- `kibanadata-dev` - Kibana data

**Memory Allocation:**
- ES: 1GB heap (`-Xms1g -Xmx1g`) - matches production docker-compose
- Kibana: Default memory limits

### ELK Certificate Generation Script (elk-setup.sh)

The init container runs this script (translated from docker-compose `setup` service):

```bash
#!/bin/bash
set -e

# Skip if certs already exist (volume cached from previous run)
if [ -f config/certs/ca/ca.crt ] && [ -f config/certs/elasticsearch/elasticsearch.crt ]; then
  echo "Certificates already exist, skipping generation..."
  exit 0
fi

echo "Generating CA..."
bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip
unzip config/certs/ca.zip -d config/certs

echo "Generating node certificates..."
cat > config/certs/instances.yml << EOF
instances:
  - name: elasticsearch
    dns:
      - elk-elasticsearch
      - elasticsearch
      - localhost
    ip:
      - 127.0.0.1
  - name: kibana
    dns:
      - elk-kibana
      - kibana
      - localhost
    ip:
      - 127.0.0.1
EOF

bin/elasticsearch-certutil cert --silent --pem \
  -out config/certs/certs.zip \
  --in config/certs/instances.yml \
  --ca-cert config/certs/ca/ca.crt \
  --ca-key config/certs/ca/ca.key

unzip config/certs/certs.zip -d config/certs

echo "Setting file permissions..."
chown -R root:root config/certs
find config/certs -type d -exec chmod 755 {} \;
find config/certs -type f -exec chmod 644 {} \;

echo "Certificate generation complete!"
echo "Elasticsearch and Kibana will handle password setup on first boot."
```

### ELK Post-Setup Script (elk-post-setup.sh)

The post-setup container waits for ES to be ready and sets the `kibana_system` password:

```bash
#!/bin/bash
set -e

echo "Waiting for Elasticsearch availability..."
until curl -s --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt https://elk-elasticsearch:9200 | grep -q "missing authentication credentials"; do
  echo "Waiting for Elasticsearch..."
  sleep 10
done

echo "Setting kibana_system password..."
until curl -s -X POST --cacert /usr/share/elasticsearch/config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" \
  -H "Content-Type: application/json" https://elk-elasticsearch:9200/_security/user/kibana_system/_password \
  -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do
  echo "Retrying kibana_system password setup..."
  sleep 5
done

echo "Elasticsearch post-setup complete!"
```

**Volume Caching Behavior:**
- First Orca run: Setup script generates certs (~60s), post-setup sets kibana_system password
- Subsequent runs: Setup exits immediately (certs exist), post-setup still runs (password is idempotent)
- To regenerate: Delete the Docker volume `docker volume rm elastic-certs`

### Aspire Init Container Pattern

The setup container uses `ContainerLifetime.Session` which means:
- Container runs when Orca starts
- Container exits after cert generation completes
- Other ELK containers `WaitFor(elkSetup)` before starting

**Alternative approach (if Session lifetime doesn't work):**
Use a custom health check that waits for cert files to exist, rather than waiting for container exit.

### Configuration Pattern (from actions.manager)

Following established token replacement pattern used by `apiClient.json` and `rabbitmq.json`:

```json
// Configurations/logging.json
{
  "logging": {
    "elasticsearchUrl": "%LOGGING_ELASTICSEARCH_URL%"
  }
}
```

**GitHub Actions replacement:**
```powershell
(Get-Content -Path ".../logging.json") |
  foreach{$_.replace('%LOGGING_ELASTICSEARCH_URL%','${{ env.LOGGING_ELASTICSEARCH_URL }}')} |
  Set-Content -Path ".../logging.json"
```

### Security Considerations

- Elasticsearch credentials should NOT be in configuration files
- Use environment variables or AWS Secrets Manager for credentials
- API key authentication preferred over username/password
- Network security: ECS → Elasticsearch connection must be allowed

---

## DevOps Requirements

**⚠️ Tasks marked with DEVOPS may require coordination or permissions beyond normal PR workflow.**

### Required GitHub Repository Changes

| Item | Repository | Type | Who Can Do |
|------|------------|------|------------|
| Add `LOGGING_ELASTICSEARCH_URL_DEV` | actions.api | Variable | Repo Admin |
| Add `LOGGING_ELASTICSEARCH_URL_STAGING` | actions.api | Variable | Repo Admin |
| Add `LOGGING_ELASTICSEARCH_URL_PROD` | actions.api | Variable | Repo Admin |
| Update `dev.yml` workflow | actions.api | PR | Developer |
| Update `staging.yml` workflow | actions.api | PR | Developer |
| Update `production.yml` workflow | actions.api | PR | Developer |

### Potential Infrastructure Changes

| Item | Impact | Effort | Notes |
|------|--------|--------|-------|
| ECS Task Definition update | Low | Small | If ES URL needs env var injection |
| Network security group | Low | Small | If ECS → ES connectivity not already allowed |
| Terraform/CloudFormation | Medium | Medium | Only if task def changes require IaC updates |

### Recommended Approach

1. **Phase 1-2 (Core + Local):** Can be completed with normal PR workflow
2. **Phase 3 (Deployment Config):** Requires GitHub repo admin to add variables
3. **Phase 4 (Testing):** Requires deployed changes from Phase 3

**Suggested Implementation Order:**
1. **Phase 1-2 (Local):** Complete core implementation + local ELK. Validate architecture with `localhost:9200`.
2. **Phase 3 MVP:** Hardcode production ES URL initially (dev/staging/prod share same cluster). Get cloud working.
3. **Phase 3 Iteration:** Add token replacement with DevOps for proper CI/CD configuration management.

---

## Testing Requirements

### Unit Testing
- Verify Elasticsearch sink configuration loads correctly
- Verify enrichers are applied to Elasticsearch-bound logs
- Verify graceful handling when Elasticsearch unavailable
- Mock Elasticsearch client for isolated testing

### Integration Testing
- Deploy to dev/staging ECS environment
- Generate ad hoc logs at various levels
- Query ELK/Kibana for new entries
- Verify no `_grokparsefailure` tags
- Verify multi-line logs (exceptions) appear as single entries
- Verify existing HTTP logging not regressed

### Manual/QA Testing
- Verify ELK Discover shows new log entries
- Filter by `data_stream.dataset: actions-api`
- Verify all log levels searchable
- Verify exception stack traces properly formatted
- Verify no duplicate entries (console + Elasticsearch)

---

## PSP Estimation (PROBE Method)

### Estimation
- **Story Points:** 16
- **Size Category:** Extra Large
- **Estimated Hours:** 32 - 48 - 64 (Optimistic - Likely - Pessimistic)

**Phase 1 - Core Implementation:** 8-12 hours
  - NuGet packages and configuration: 1-2 hours
  - Logging config file + Configuration.Logging.cs: 2-3 hours
  - Async wrapper + destructuring limits: 1-2 hours
  - Environment-specific console/sink filtering: 2-3 hours
  - Format matching with application-logs: 1-2 hours
  - Performance benchmarking: 1-2 hours

**Phase 2 - Local Development:** 2-4 hours
  - Local ELK stack version update: 1-2 hours
  - Local dev config (logging.Development.json): 0.5-1 hour
  - Local testing and documentation: 0.5-1 hour

**Phase 3 - Deployment Configuration:** 2-4 hours
  - GitHub variables setup: 0.5-1 hour
  - Workflow updates (dev/staging/prod): 1-2 hours
  - ECS task definition (if needed): 0.5-1 hour

**Phase 4 - Integration Testing:** 4-6 hours
  - Local development testing: 1-2 hours
  - ECS environment testing: 2-3 hours
  - Production validation: 1-2 hours

**Phase 5 - Orca ELK Orchestration (Full Stack):** 16-24 hours
  - Task 15 - ELK Setup/Init container with cert generation: 4-6 hours
    - Cert generation script translation from docker-compose
    - Volume caching implementation
    - Health check for cert existence
  - Task 16 - Elasticsearch with TLS: 2-3 hours
    - Full security configuration
    - Health checks
  - Task 17 - Kibana with TLS: 2-3 hours
    - ES connection via HTTPS
    - kibana_system user setup
  - Task 18 - Logstash: 2-3 hours
    - Config file migration
    - ES connection
  - Task 19 - Filebeat: 2-3 hours
    - Config file migration
    - Log path mounts
  - Task 20 - Documentation and integration testing: 4-6 hours
    - Full stack testing (fresh + cached)
    - README updates

- **Confidence:** Medium (init container cert generation is the highest-risk task)
- **Estimation Date:** 2025-12-05 (updated 2025-12-10)
- **Estimation Notes:** Scope expanded to include full ELK stack (ES, Kibana, Logstash, Filebeat) with TLS/auth enabled via Aspire. Init container pattern for cert generation with volume caching. This makes Orca self-contained for local ELK - no dependency on resolve.dev.resources.

### Tracking
- **Started:**
- **Completed:**
- **Actual Hours:**
- **Estimation Accuracy:**

---

## Implementation Progress

### Branch: `feature/PLAT-3239-elk-logging`

### Commits (oldest to newest)
1. `b9789f5` Add Serilog packages for ELK logging
2. `b6ef14c` Configure dual-sink async logging for ELK
3. `7f96a63` Match production index format (no date rotation)
4. `5d6388b` Use HTTP sink with custom formatter for filebeat-compatible ES output
5. `8ed2d72` Consolidate Serilog configuration
6. `7bd1407` Add request context enrichment via middleware for ES logs
7. `deabb04` Add TenantToken auth support and normalize tenant logging properties
8. `060faaa` Add caching to resolve both Tenant and TenantId for all auth types
9. `5269ea2` Fix middleware order so exception logs include tenant context
10. `d7af216` Add Slack webhook tenant extraction to logging middleware
11. `bd08201` Add Serilog logging benchmark suite (with drop monitoring)

### Implementation Pivot: HTTP Sink Instead of ES Sink

**Why:** The `Serilog.Sinks.Elasticsearch` package created index mapping conflicts with the existing `application-logs` index. The custom HTTP sink with `FilebeatCompatibleFormatter` produces PascalCase JSON that matches the existing filebeat/logstash pipeline exactly.

### New Components Created

| File | Description |
|------|-------------|
| `TenantInfoCache.cs` | 24-hour TTL cache for tenant name↔ID mapping lookups |
| `TenantUserLoggingMiddleware` | Captures Tenant/TenantId/UserId at request start via LogContext |
| `FilebeatCompatibleFormatter` | PascalCase JSON formatter matching filebeat output |
| `ElasticsearchBulkFormatter` | Formats batches for ES `/_bulk` API |
| `ElasticsearchHttpClient` | HTTPS client with basic auth, self-signed cert support |

### Auth Scheme Support Implemented

| Auth Type | Tenant Source | TenantId Source | Status |
|-----------|---------------|-----------------|--------|
| JWT/Keycloak (Bearer) | Issuer URL realm name | Cache lookup | ✅ Done |
| TenantToken (Token) | Cache lookup | Decrypted token | ✅ Done |
| Slack Webhook | Cache lookup | URL path `/api/Webhooks/postEvent/{tenantId}` | ✅ Done |
| Basic Auth Webhook | Cache lookup | URL path | ✅ Done |

### What's Complete
- [x] Dual-sink architecture (Console for HTTP logs + HTTP sink for all logs to ES)
- [x] FilebeatCompatibleFormatter for PascalCase JSON matching existing pipeline
- [x] TenantUserLoggingMiddleware for request-scoped context
- [x] JWT/Keycloak auth support (tenant from issuer URL)
- [x] TenantToken auth support ("Token" scheme, TenantId from decrypted token)
- [x] Slack webhook tenant extraction from URL path
- [x] TenantInfoCache with 24-hour TTL for name↔ID resolution
- [x] Middleware order fix so exception logs include tenant context

### What Remains
- [ ] End-to-end validation in staging environment
- [ ] Verify logs appear correctly in ELK with all auth types
- [ ] PR review and merge to staging
- [ ] Production deployment

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-05 | 1.0 | Initial story creation from PLAT-3239 with codebase exploration | Prism Dev Agent |
| 2025-12-05 | 1.1 | Changed package from Elastic.Serilog.Sinks to Serilog.Sinks.Elasticsearch - ES cluster is 7.17.15 | Prism Dev Agent |
| 2025-12-05 | 1.2 | Dual-sink architecture: Console (HTTP only) + ES sink (ad hoc only). Target index: `application-logs`. | Prism Dev Agent |
| 2025-12-05 | 2.0 | **Major scope expansion:** Added local dev environment setup, GitHub Actions config, environment-specific behavior. Reorganized into 4 phases (14 tasks). Story points: 3→8. Added DevOps requirements section. | Prism Dev Agent |
| 2025-12-05 | 2.1 | **Phase 2 simplification:** actions.api uses ES sink in ALL environments (not Filebeat). Behavior identical across local/ECS - only ES URL differs. Tasks reduced: 14→13. Updated architecture diagrams. | Prism Dev Agent |
| 2025-12-05 | 2.2 | **Performance & memory protection:** Added `Serilog.Sinks.Async` for non-blocking logging. Added destructuring limits to prevent large exception memory pressure. Added Task 6 (Performance Benchmarking). Tasks: 13→14. | Prism Dev Agent |
| 2025-12-09 | 3.0 | **Implementation started.** Pivoted from `Serilog.Sinks.Elasticsearch` to `Serilog.Sinks.Http` with custom formatter for direct ES bulk API compatibility. 10 commits on feature branch. | Prism Dev Agent |
| 2025-12-09 | 3.1 | **Task 6 partial:** Created benchmark suite (Logging.Benchmarks project). Includes BenchmarkDotNet, quick CLI tests, ES-specific benchmarks with MockHttpServer, and tuning benchmarks for bufferSize/blockWhenFull. Large exception and buffer overflow tests pending. | Prism Dev Agent |
| 2025-12-09 | 3.2 | **Task 6 stress tests complete:** Added StressBenchmarks.cs with large exception test (100 iterations, 20-level deep exceptions, 100KB data - PASS: 1,687 exc/sec, memory bounded) and buffer overflow test (10K logs with slow server - PASS: 391K logs/sec non-blocking, memory bounded). Documentation still pending. | Prism Dev Agent |
| 2025-12-10 | 3.3 | **Benchmark suite refactored:** Complete rewrite with BenchmarkRunner, SinkFactory, LogDropMonitor (IAsyncLogEventSinkMonitor), MockHttpServer. Generates JSON + Markdown reports. Drop monitoring tracks buffer overflow events. Throughput: 157K-617K logs/sec depending on sink. Commit: bd08201. | Claude Code |
| 2025-12-10 | 4.0 | **Scope expansion: Orca ELK orchestration.** Added Phase 5 (Tasks 15-19) to automatically start local ELK stack via .NET Aspire when running Orca. ES + Kibana containers managed alongside Redis, RabbitMQ, Keycloak. Story points: 8→13. | Story Master |
| 2025-12-10 | 4.1 | **Phase 5 expanded to full ELK stack.** Tasks 15-20 now include: init container for TLS cert generation (with volume caching), ES with full security, Kibana, Logstash, Filebeat. Auth enabled to match production. Story points: 13→16. Orca becomes self-contained for ELK (no resolve.dev.resources dependency). | Story Master |
| 2025-12-10 | 4.2 | **Local dev only.** ELK containers wrapped in `IsDevelopment() && !CI && !GITHUB_ACTIONS` conditional. Cloud dev environments use production ELK stack. | Story Master |
| 2025-12-10 | 4.3 | **Phase 5 implementation complete.** Updated story to reflect final implementation: container naming (`elk-` prefix), two-phase setup (elk-setup + elk-post-setup), credentials (`R3solv3!`), cert paths (`elasticsearch/elasticsearch.*`), file permissions (755/644), `WithParentRelationship` for Aspire dashboard grouping. All Phase 5 tasks marked complete. | Story Master |

---

## Dev Agent Record

### Agent Model Used
- Story Creation: Opus 4.5

### File List

**Phase 1 - actions.api Core:**
1. `D:\dev\actions.api\src\actions.api.csproj` - Add NuGet packages
2. `D:\dev\actions.api\src\Configurations\logging.json` - New config file with token replacement
3. `D:\dev\actions.api\src\Configurations\Configuration.Logging.cs` - New config loader
4. `D:\dev\actions.api\src\Configurations\Configuration.Logger.cs` - Update for environment-specific filtering
5. `D:\dev\actions.api\src\Configurations\logger.json` - Update with ES sink configuration

**Phase 2 - Local Development:**
6. `D:\dev\resolve.dev.resources\Docker Containers\elastic-stack\sample.env.txt` - Update STACK_VERSION
7. `D:\dev\actions.api\src\Configurations\logging.Development.json` - Local ES URL (localhost:9200)

**Phase 3 - GitHub Actions:**
8. `D:\dev\actions.api\.github\workflows\dev.yml` - Add logging config replacement
9. `D:\dev\actions.api\.github\workflows\staging.yml` - Add logging config replacement
10. `D:\dev\actions.api\.github\workflows\production.yml` - Add logging config replacement

**Benchmarks (Task 6):**
11. `D:\dev\actions.api\benchmarks\Logging.Benchmarks\Logging.Benchmarks.csproj` - Benchmark project
12. `D:\dev\actions.api\benchmarks\Logging.Benchmarks\Program.cs` - CLI entry point
13. `D:\dev\actions.api\benchmarks\Logging.Benchmarks\BenchmarkRunner.cs` - Core test runner with throughput/stress/burst tests
14. `D:\dev\actions.api\benchmarks\Logging.Benchmarks\SinkFactory.cs` - Factory creating loggers with LogDropMonitor
15. `D:\dev\actions.api\benchmarks\Logging.Benchmarks\Infrastructure.cs` - NullSink, MockHttpServer, LogDropMonitor
16. `D:\dev\actions.api\benchmarks\Logging.Benchmarks\ReportGenerator.cs` - JSON + Markdown report generation
17. `D:\dev\actions.api\actions.api.sln` - Solution file updated with benchmark project
18. `D:\dev\actions.api\.gitignore` - Updated to exclude benchmark results

**GitHub Repository Settings (⚠️ DEVOPS):**
- Variable: `LOGGING_ELASTICSEARCH_URL_DEV`
- Variable: `LOGGING_ELASTICSEARCH_URL_STAGING`
- Variable: `LOGGING_ELASTICSEARCH_URL_PROD`

**Deployment Files (if needed):**
19. ECS Task Definition - Environment variable injection

**Phase 5 - Orca ELK Orchestration (Full Stack):**
20. `D:\dev\orca\Orca.AppHost\Program.cs` - Add full ELK stack (setup, post-setup, ES, Kibana, Logstash, Filebeat)
21. `D:\dev\orca\Orca.AppHost\elk-setup.sh` - Certificate generation script
22. `D:\dev\orca\Orca.AppHost\elk-post-setup.sh` - Post-setup script for kibana_system password
23. `D:\dev\orca\Orca.AppHost\elk\logstash.conf` - Logstash pipeline config (copied from resolve.dev.resources)
24. `D:\dev\orca\Orca.AppHost\elk\filebeat.yml` - Filebeat config (copied from resolve.dev.resources)
25. `D:\dev\orca\README.md` - Document ELK stack information

### Debug Log References
(To be populated by dev agent if debugging required)

### Completion Notes
(To be populated upon completion)

---

**Story Status:** Draft - Pending refinement and approval before implementation

---

## Sources

- [Serilog.Sinks.Elasticsearch GitHub](https://github.com/serilog-contrib/serilog-sinks-elasticsearch) - Package documentation and configuration options
- [Serilog.Sinks.Elasticsearch NuGet](https://www.nuget.org/packages/Serilog.Sinks.Elasticsearch) - Latest package version
- [Elastic.Serilog.Sinks](https://www.elastic.co/docs/reference/ecs/logging/dotnet/serilog-data-shipper) - Future migration target when ES upgrades to 8.x
